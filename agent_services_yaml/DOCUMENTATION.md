# Introduction

This is the general documentation for the SGP agent builder. It covers general features available as part of the agent builder YAML language as well as the specific Nodes, Types, Processing Functions, and Example Configs available to those building applications. This is still very much a work in progress and much of it is autogenerated so please report rough edges as you find them. This page contains general information on the agent builder YAML language and development flow. Refer to the other pages linked below for documentation on Nodes, Types, Processing Functions, and Example Configs.

### Table of Contents

1. [Nodes](nodes.html)
2. [Processing Functions](nodes/processing/functions.html)
3. [Types](types/core.html)
4. [Example Configs](documentation/examples.html)
5. [High Level Concepts](#high-level-concepts)
   - [Workflows: Directed Acyclic Graphs](#workflows-directed-acyclic-graphs)
   - [State Machines](#state-machines)
6. [General Agent Builder Features](#general-agent-builder-features)
   - [Input Typing](#input-typing)
   - [User Input Schema Validation](#user-input-schema-validation)
   - [Reconciling User-Specified Information with Generated Graph Inputs](#reconciling-user-specified-information-with-generated-graph-inputs)
   - [Using Graph Inputs](#using-graph-inputs)
   - [Example](#example)
   - [Output Typing via Channels](#output-typing-via-channels)
   - [Processing Node](#processing-node)
   - [Tool Generation](#tool-generation)
   - [Fan-out for workflow operations](#fan-out-for-workflow-operations)

### High Level Concepts

Agent Builder envisions LLM applications as graphs where nodes are configurable, stateless _functions_. While these functions can be arbitrary, we assume most common applications are compositions of a restricted set of nodes, e.g. LLM completions, knowledge base retrieval, re-ranking, etc. For details on existing node types, please refer to.

#### Workflows: Directed Acyclic Graphs

The simplest application-as-a-graph is a _workflow_. A workflow is just a stateless, directed acyclic graph with a deterministic execution order. It represents an application with a fixed set of inputs and outputs. To specify a workflow graph, a user must provide a YAML configuration file specifying the configuration of each node, as well as their input-output connections.

Here is an example YAML for a simple RAG application:

```yaml

account_id: ... # you SGP account ID goes here
workflow:
  # gets the last user text from a list of messages
  - name: "get_question" # you can name your nodes arbitrarily, but names must be unique!
    type: "get_message" # this is the type of the node. for available types, refer to `NodeRegistry().registry` or `nodes.md`
    config:
      index: -1
    inputs: # these are mappings from abstract node inputs to graph inputs
      messages: "messages" # messages argument should be obtained from the messages variable

  # retrieve the top 100 artifacts from an SGP KB using the extracted user text
  - name: "retrieve"
    type: "retriever"
    config: # these are the parameters of the node instance
      knowledge_base_name: "egp_services_retrieval_demo"
      num_to_return: 100 # only get the top 100 chunks
    inputs:
      query: "get_question.output" # query is obtained from the node get_question's output -- hence, the suffix .output

  # rerank the retrieved artifacts
  - name: "rerank"
    type: "reranker"
    config:
      num_to_return: 10 # only top 10 now
      scorers:
        - name: "cross-encoder"
          model: "cross-encoder/ms-marco-MiniLM-L-12-v2"
    inputs:
      query: "get_question.output" # we need the original query AND the retrieved chunks to re-rank
      chunks: "retrieve.output"

  # format the re-ranked chunks and original question into a single text prompt
  - name: "prompt"
    type: "jinja"
    config:
      data_transformations:
        context_chunks:
          jinja_template_str: "{% for chunk in value %}{{ chunk.text }}\n{% endfor %}"
      output_template: # you can provide a string directly or a path to a template
        jinja_template_path: "egp_services/prompt_templates/default_prompt_template_query.jinja"
    inputs:
      context_chunks: "rerank.output"
      question: "get_question.output"

  # call an LLM with the prompt we created
  - name: "llm"
    type: "generation"
    config:
      model: "gpt-3.5-turbo"
      max_tokens: 512
      temperature: 0.2
      node_metadata:
        - _time_taken
        - _input_token_usage
        - _output_token_usage
    inputs:
      input_prompt: "prompt.output"

  # extract a citation from the llm's output
  - name: "response"
    type: "citation"
    config:
      citation_type: "rouge"
      citation_context:
        metric: "rouge2"
        score: "recall"
        min_similarity: 0.4
    inputs:
      generated_text: "llm.output"
      chunks: "rerank.output"

final_output_nodes: ["response"] # the names of the nodes to include in the app's output dict
```

To run this config, assuming the above was stored at `example_config.yaml`, you could create and run the application in the SGP application builder UI.

#### State Machines

**Directed Graphs with State:** Not every application has a determistic execution order. To support general control flow, such as conditional routing and looping, EGP Services provides a second abstraction called a StateMachineApp. You can think of this as a "state", i.e. an arbitrary key-value store, and a directed graph of "actions", where each action is a workflow. While each action is deterministically executed, it can map to other actions conditionally. The syntax for a `StateMachineApp` config is as follows:

```yaml
initial_state: # define the schema and default values in your state dictionary
  messages: []

starting_node: get_user_message # the first action to perform

machine:
  get_user_message:  # the name of the action
    workflow_config:
      streaming_nodes: [] # an optional list of nodes to stream from
      workflow: # this is the same as a workflow config!
        - name: get_new_user_message
          type: create_messages
          config:
            message_configs:
              - role: user
                content: user_message
          inputs:
            user_message: query

        - name: add_user_message
          type: insert_messages
          config:
            index: -1
          inputs:
            messages: messages
            new_messages: get_new_user_message.output

    write_to_state: # this tells the machine how to update the state based on workflow outputs
      messages: add_user_message

    next_node: # this configures the next action to take

      cases: # conditionally route to nodes based on conditions
        - value: __end__
          condition:
            condition_input_var: query
            operator: contains
            reference_var: 'done'

      default: basic_chat # or route to a default action

  basic_chat:
    workflow_config:
      final_output_nodes: ["llm_call"]
      workflow:
        - name: llm_call
          type: chat_generation
          config:
            model: llama-3-1-70b-instruct
            max_tokens: 512
            temperature: 0.0
            memory_strategy:
              name: "last_k"
              params:
                k: 1000
          inputs:
            messages: messages

        - name: get_new_llm_message
          type: create_messages
          config:
            message_configs:
              - role: assistant
                content: llm_response
          inputs:
            llm_response: llm_call.output

        - name: add_llm_message
          type: insert_messages
          config:
            index: -1
          inputs:
            messages: messages
            new_messages: get_new_llm_message.output

    write_to_state:
      messages: add_llm_message

    next_node:
      default: __end__ # when the app is finished, it should map to this action

```

State machine configs can also be run from the SGP Application Builder UI.


## General Agent Builder Features

In order to display inputs and outputs of your applications in the SGP UI, you need to add additional information in the application configuration that helps the frontend display these inputs and outputs appropriately given their type.

#### Input Typing

Each node defines the expected inputs and outputs. While some nodes collect inputs from preceding nodes, others may require direct user input to execute. For user inputs, you can specify additional details such as expected type, description, default values, and requirements. All of this can be configured within the `user_input` section of the application configuration.

**NOTE**: Adding input typing will cause your application to *interrupt* at the level you specifiy that input type and ask the user for that value. For example, if you define `user_input` at the machine level, it will interrupt before running any state to ask the user for that value. If you define it at the workflow level, it will only interrupt at the beginning of that workflow.

```yaml
# This is WRONG because I don't want to interrupt basic_chat and ask for user_question
user_input:
  user_question: 
    type: ShortText

machine:
  get_user_message:  
      ...
      workflow:
        - name: get_new_user_message
          ...
          inputs:
            user_message: user_question
    ...
    next_node:
      default: basic_chat

  basic_chat:
      ...
      workflow:
        - name: llm_call # pulls from state
     ...
    next_node:
      default: get_user_message
```

```yaml
machine:
  get_user_message:
    # This is right because I only want to interrupt get_user_message to ask for user_question and not basic_chat
    user_input:
      user_question:
        type: ShortText 
      ...
      workflow:
        - name: get_new_user_message
          ...
          inputs:
            user_message: user_question
    ...
    next_node:
      default: basic_chat

  basic_chat:
      ...
      workflow:
        - name: llm_call # pulls from state
     ...
    next_node:
      default: get_user_message
```

##### User Input Schema Validation

The user input in a config adheres to [the standard JSON schema](https://json-schema.org/understanding-json-schema/about). The key specifications include `properties` and `required`. Each property can define details such as title, description, and type, all in accordance with standard JSON schema. The input validator ensures compliance with this format, verifying that types are either standard primitive types (e.g., `"integer"`, `"number"`, `"string"`, `"boolean"`, `"null"`, `"array"`, `"object"`) or custom shared classes defined in EGP core types (e.g., `"Message"`, `"KnowledgeBaseId"`, `"KnowledgeBaseIds"`, etc.). An error will be raised if the specified type does not match. Properties without a specified default are considered required.

Example valid user input:

```json
{
  "properties": {
      "rag_query": {
          "title": "rag query title",
          "description": "rag query description",
          "type": "ShortText",
      },
      "knowledge_base_ids": {
          "title": "kb title",
          "description": "kb description",
          "type": "KnowledgeBaseIds"
      },
  },
  "required": [
      "rag_query", "knowledge_base_ids"
  ]
}
```

##### Reconciling User-Specified Information with Generated Graph Inputs

If the user-provided information lacks type details, the type automatically generated from the signature will be used. If the type specified in the user inputs is incompatible with the type signature, an error will be raised.

##### Using Graph Inputs

1. Fetching All Graph Inputs
   Retrieves all necessary inputs required for the workflow to execute.

2. State machine interrupts
   Returns the same set of inputs but excludes those already received.

##### Example

Config:

```yaml
user_input:
  properties:
    rag_query:
      title: "rag query title"
      description: "rag query description"
      type: "ShortText"
    knowledge_base_ids:
      title: "kb title"
      description: "kb description"
      type: "KnowledgeBaseIds"
  required:
    - "rag_query"

workflow:
  - name: "retriever"
    type: "retriever"
    config:
      num_to_return: 100
    inputs:
      query: "rag_query"
      knowledge_base_ids: "knowledge_base_ids"
```

Missing inputs:

```json
{
    "title": "missing_inputs",
    "type": "object",
    "$defs": {
        "ShortText": {
            "title": "ShortText",
            "type": "string",
            "description": "Represents a short text input, typically used for titles or headings.",
        },
        "KnowledgeBaseIds": {
            "title": "KnowledgeBaseIds",
            "type": "array",
            "$defs": {
                "KnowledgeBaseId": {
                    "title": "KnowledgeBaseId",
                    "type": "string",
                }
            },
            "items": {"$ref": "#/$defs/KnowledgeBaseId"},
        },
    },
    "properties": {
        "knowledge_base_ids": {
            "$ref": "#/$defs/KnowledgeBaseIds",
            "title": "kb title",
            "description": "kb description"
        },
        "rag_query": {
            "$ref": "#/$defs/ShortText",
            "title": "rag query title",
            "description": "rag query description"
        },
    },
    "required": ["rag_query"]
}
```

#### Output Typing via Channels

Output channels are named and typed buffers. Every final/streaming node output will by default be cast to a channel type for communication with the SGP platform. By default, the channel name will be unique to the node, but users can choose to assign multiple nodes to the same channel. 
Each node, however, can only be assigned to a single channel. Channel types are either inferred by the output signature of the nodes that feed into it or can be specified by the user. If the channel type cannot be inferred and is not provided by the user, it will default to a `default_channel_type`, 
which is specifiable in the config. `default_channel_type` itself defaults to the `AnyChannel` type -- this is a simple buffer where elements are collected into a list. 

To see the available channel types you can refer to `egp_services/types/core/__init__.py` or the `ChannelRegistry` therein. These all inherit from `BaseChannel` and are equipped with a `combine` method
that indicates how to aggregate elements of that type.  

To specify channels in your config, you need to provide a spec in the `final_output_nodes` or `streaming_nodes` fields:
```yaml
streaming_nodes: 
  - node: "llm"
    channel_name: "message"
    channel_type: "message"
    cast_mode:
      mode: "direct"
      target_key_map: "content"
      target_model_kwargs:
        role: assistant
  
  - node: "llm_citation"
    channel_name: "citation"
    channel_type: "citation_response"
    cast_mode:
      mode: serialize
      target_model_kwargs:
        type: citation_response
          
final_output_nodes:
  - node: "thought"
    channel_name: "thought"
    channel_type: "intermediate_text"
  
  - node: "prompt"
    channel_name: "prompt"
    channel_type: "text"

default_channel_type: "output_text"
```


##### Processing Node

The ProcessorNode is a flexible component that lets you execute a series of custom functions in a specific order. Think of it as a pipeline builder that can:
1. Chain multiple functions together where outputs from one function become inputs to another
2. Process data through custom logic specific to your project needs
3. Execute functions in the correct dependency order automatically

Core Concepts

1. **Functions**: Custom code pieces that perform specific operations
2. **Function Specs**: Definitions of functions and their parameters
3. **Return Key**: Specifies which function's output becomes the final result


How to Use It: 

###### Write your custom code

**NOTE**: Your functions must have typed inputs and outputs.

(please check if it already exists first!)

```python
from egp_services.nodes.processing.decorator import register_function

@register_function("special_readme_fn")
def special_readme_fn(test_input: str) -> str:
    """
    "key" -> "README: key"
    """
    return "README:" + test_input
```

######  Define Your Function Specs

For each function you want to use, create a specification that includes the path to the function and the arguments (kwargs) the function needs. Below is an example of *one* function spec:

```yaml
my_function_name: # this is the name in your processor config, it can be whatever
  path: special_readme_fn # this must be a registered function name
  kwargs: # dictionary of all inputs in special_readme_fn function header
    test_input: input_from_user # this maps the processor's input to test_input
```

######  Chain Functions Together
You can pass the output of one function as input to another by referencing the first function's name in the second function's kwargs.

```yaml
second_function_name: # this is a second function spec
...
  kwargs:
    second_fn_input: my_function_name # referencing an input from another function_spec
```

######  Specify a Return Key
Tell the processor node which function's output should be returned as the final result.

```yaml
config:
  return_key: second_function_name
  function_specs:
    ...
```


######  Example Processor Node Configuration (Putting it all together)

```yaml
type: processor # the type of the node
inputs:
  processors_input_name: input_to_workflow # processor_input_name is used within the node, input_to_workflow is the input coming into the node (e.g. from the user args)
config: # the processor config
  return_key: sequence_dropout # the final key outputted by this node
  function_specs: # the list of functions that are being chained together
    # First function converts sequence to dictionary
    load_chunks_to_dict: # the name of the function within processor node
      path: sequence_pydantic_to_dict # the name of the already registered function
      kwargs:
        sequence: processors_input_name # sequence is required as input to sequence_pydantic_to_dict
    
    # Second function extracts JSON from the dictionary
    extract_nested_json:
      path: extract_nested_json
      kwargs:
        key: text.numbers # text.numbers is a raw string that's the input to the argument key
        jsons: load_chunks_to_dict  # Uses result from previous function
    
    # Third function shuffles the extracted data
    sequence_shuffle:
      path: sequence_shuffle
      kwargs:
        sequence: extract_nested_json  # Uses result from previous function
    
    # Final function - also the return_key
    sequence_dropout:
      path: sequence_dropout
      kwargs:
        sequence: sequence_shuffle  # Uses result from previous function
        dropout_rate: 0.5 # a float passed in directly to this function 
```


###### Key Features

* Automatic Dependency Resolution: The node figures out the correct order to execute functions
* Type Checking: Ensures all functions receive the correct types of arguments
* Circular Reference Detection: Prevents infinite loops by detecting circular dependencies
* Flexible Input Sources: Function arguments can come from node inputs, configuration values, or outputs of other functions


#### Tool Generation

EGP Servies has an "[oldowan](https://github.com/scaleapi/models/tree/master/enterprise/oldowan)"-enabled node called `ToolGeneration` which makes it very easy to loop in tools into a chat generation nodes.

The node is described [here](https://github.com/scaleapi/models/blob/master/enterprise/egp_services/egp_services/nodes/generation/tool_generation.py) and some example configs [here](https://github.com/scaleapi/models/tree/master/enterprise/egp_services/examples/oldowan).

Below is a sample tool generation node:
```yaml
- name: "google_search_example"
  type: "tool_generation"
  config:
    model: "openai/gpt-4o"
    tools:
      - name: "internal.google_search"
  inputs: # these are mappings from abstract node inputs to graph inputs
    messages: "messages" # messages argument should be obtained from the messages variable
```

You can also bring your own tool by either writing your own python function or leveraging your own node and then adding it [here](https://github.com/scaleapi/models/blob/master/enterprise/egp_services/egp_services/nodes/generation/tool_generation.py#L30). 

Below is usage of tools located arbitrarily in one's filesystem with the ToolGenerationNode:

```python
from egp_services.nodes.generation.tool_generation import ToolGenerationNode

# 3 options to specify tools:
# 1. tools in `oldowan.tools` via `name`, e.g `internal.google_search`
# 2. nodes in `egp_services` via `name`, e.g. `nodes.CodeExecutionNode`
# 3. arbitrary modules via `path`

tools = [{'name' : 'nodes.CodeExecutionNode', 'init_kwargs': {'files' : {}}}, 
         {'name' : 'nodes.ChatGenerationNode', 'init_kwargs': {'llm_model' : 'gpt-4o-mini'}},
         {'name' : 'internal.google_search', 'init_kwargs': {}},
         {'path' : 'customers.internal_se.processing.functions.wiki_search'}]

node = ToolGenerationNode(name='', model='openai/gpt-4o-mini', tools=tools)
output = node.stream_run_aggregated(messages=[{"role" : "user", "content" : "Look up wikipedia and tell me what K-Medoids is"}]) 

for chunk in output:
  print(chunk)
```

Using a custom node:

```python
from egp_services.nodes.generation.tool_generation import ToolGenerationNode

node = ToolGenerationNode(name='', model='openai/gpt-4o-mini', 
                          tools=[{'name' : 'nodes.CodeExecutionNode', 'init_kwargs': {'files' : {}}}, 
                          {'name' : 'nodes.ChatGenerationNode', 'init_kwargs': {'llm_model' : 'gpt-4o-mini'}}])

output = node.run(messages=[{"role" : "user", "content" : "Please generate an implementation of 2sum and then ask claude-3.5-sonnet if your implementation is correct"}])
```

#### Fan-out for workflow operations

Workflow operations can be applied ("fanned-out") over the elements of their inputs. This feature is invoked by adding an indexing expression to the end of one or more input arguments in the workflow's configuration. The specified indices can be any valid variable name and multiple can be specified with comma separation. When multiple are present, they are treated in sorted order.

For example, imagine we want to translate the input text. The following workflow config, expressed as YAML, generates a message we can send to an LLM to translate the input text.

```yaml
name: format_message
type: jinja
config:
  output_template:
    jinja_template_str: |-
       Please translate the following text to Spanish:
       
      {{ text }}
inputs:
  text: input_text
```

Suppose we want to translate a batch of inputs instead. We can process a list of input texts simply by specifying `text: input_texts@i` in the `inputs`.

```yaml
name: format_messages
type: jinja
config:
  output_template:
    jinja_template_str: |-
       Please translate the following text to Spanish:
       
      {{ text }}
inputs:
  text: input_texts@i
```

Additionally, if we have a second list of target languages, we can zip the two lists by specifying the same index on both inputs. This uses Python's `zip`, so the result will be as long as the shortest of the inputs.

```yaml
name: format_messages
type: jinja
config:
  output_template:
    jinja_template_str: |-
       Please translate the following text to {{ target_language }}:
       
      {{ text }}
inputs:
  text: input_texts@i
  target_language: corresponding_languages@i
```

Furthermore, we could generate a pairwise translation by using different indices.

```yaml
name: format_messages
type: jinja
config:
  output_template:
    jinja_template_str: |-
       Please translate the following text to {{ target_language }}:
       
      {{ text }}
inputs:
  text: input_texts@i
  target_language: languages@j
```

Because the indices are treated in sorted order, the output `format_messages.output[i][j]` will be the translation of `input_texts[i]` to language `languages[j]`.

In this final case, to pass all the formatted messages to an LLM, we need to expand both indices. This can be accomplished by using input `prompt: format_messages.output@i,j`. (Since the indices are arbitrary, you could just as well use `prompt: format_messages.output@bar, foo` but they must appear in sorted order so `prompt: format_messages.output@foo, bar` is not valid. Note, too, that whitespace around commas is stripped from the index names.)

For a more complete example, let's examine a workflow that computes a foo-bar multiplication table. We will accept an input list of numbers, compute a multiplication table over those numbers, and replace values following the usual foo-bar rules (foo for multiples of 3 and bar for multiples of 5; therefore foobar for multiples of 15).

```yaml
workflow:
  - name: multiplication-table
    type: jinja
    config:
      output_template:
        jinja_template_str: "{{ row * col }}"
    inputs:
      row: numbers@i
      col: numbers@j
  - name: to-number
    type: data_transform
    config:
      action: json_loads
    inputs:
      response: multiplication-table.output@i,j
  - name: foobar
    type: jinja
    config:
      output_template:
        jinja_template_str: "{{ 'foobar' if number % 15 == 0 else 'foo' if number % 3 == 0 else 'bar' if number % 5 == 0 else number }}"
    inputs:
      number: to-number.output@i,j
  - name: foobar-table
    type: jinja
    config:
      output_template:
        jinja_template_str: |-
          {% for row in foobar %}
          {% for word in row %}{{ '{:>7s}'.format(word) }}{% endfor %}
          
          {% endfor %}
    inputs:
      foobar: foobar.output
 ```

We get a pairwise expansion of `nunbers` by indexing over two different indices, performing multiplication within a jinja node for convenience then loading the results as numbers using `json_loads` over `multiplication-table.output@i,j`. We then substitute values as needed in another jinja node and, lastly, use a final jinja node to format them into a table. (This all could be accomplished with a single jinja node using a nested for loop, but the point is to demonstrate the fan-out operation, which would work for logic that can't be expressed in a jinja node, such as LLM calls.)

Given the inputs `{"numbers": [1, 2, 3, 4, 5, 6]}`, this workflow outputs:

```python
      1      2    foo      4    bar    foo
      2      4    foo      8    bar    foo
    foo    foo    foo    foo foobar    foo
      4      8    foo     16    bar    foo
    bar    bar foobar    bar    bar foobar
    foo    foo    foo    foo foobar    foo
```

Under the hood, these nodes are wrapped in a `FanOutNode`, which uses the wrapped node's `batch_run` when not streaming, and when streaming uses a `ThreadPoolExecutor` to run each iteration of the fanned-out operation and yield them as they are available. Note this does not support streaming of the wrapped operation, e.g., if you fan-out a generation node, the streamed results will be the full output text for each item in the input. This also means that streaming only yields over the first of multiple indices (e.g., for operation "foobar" in the example above, if we streamed those outputs to the caller, they would receive one full column of the table at a time.)